{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panda Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('lib')\n",
    "\n",
    "import scipy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time, os\n",
    "from IPython.display import clear_output\n",
    "from IPython.core import display\n",
    "\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import networkx as nx\n",
    "\n",
    "import pinocchio as pin\n",
    "from utils import *\n",
    "#from costs import *\n",
    "from robot import *\n",
    "from costs_pseudo import *\n",
    "from functools import partial\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf1\n",
    "from collections import OrderedDict\n",
    "import tf_robot_learning as rl\n",
    "import tf_robot_learning.distributions as ds\n",
    "from tf_robot_learning import kinematic as tk\n",
    "\n",
    "DATA_PATH = '/home/teguh/git/publications/learning_distribution_gan/tf_robot_learning/data'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Define and Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf1.InteractiveSession()\n",
    "tf1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define the robot as a kinematic chain, loaded from urdf\n",
    "urdf = tk.urdf_from_file(DATA_PATH + '/urdf/panda_arm.urdf');\n",
    "chain = tk.kdl_chain_from_urdf_model(urdf, tip='panda_hand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/panda_constrained_orientation.npy')[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_augmented(q):\n",
    "#     return q\n",
    "    return tf.concat([q, fs[0](q)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mini batch\n",
    "def get_batch(_batch_size=30, cut=None, augmented=True):\n",
    "    if cut is not None: idx = np.random.randint(0, cut, _batch_size)\n",
    "    else: idx = np.random.randint(0, data_augmented.shape[0]-1, _batch_size)\n",
    "    if augmented: return data_augmented[idx]\n",
    "    else: return data[idx]\n",
    "    \n",
    "def get_target_batch(_batch_size=30, cut=None):\n",
    "    if cut is not None: idx = np.random.randint( 0, cut, _batch_size)\n",
    "    else: idx = np.random.randint(0, data_augmented.shape[0]-1, _batch_size)\n",
    "    return data_target[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformations of interest (task-spaces)\n",
    "fs = [\n",
    "    lambda q : chain.xs(q)[:, -1], # get position and orientation of end-effector\n",
    "    lambda q : chain.xs(q)[:, -1, :3], # get position end-effector\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utilities to evaluate these functions in tensorflow\n",
    "q_eval = tf1.placeholder(tf.float32, (None,7))\n",
    "x_eval = chain.xs(q_eval)\n",
    "           \n",
    "q_augmented_eval = q_augmented(q_eval)\n",
    "q_target_eval = fs[1](q_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute data through these transformations\n",
    "data_augmented = q_augmented_eval.eval({q_eval: data})\n",
    "data_target = q_target_eval.eval({q_eval: data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "joint_dim = 7 # panda joint angles\n",
    "latent_dim = 10 # dimension of noise\n",
    "\n",
    "target_dim = 3  # size of the target\n",
    "\n",
    "augmented_dim = data_augmented.shape[-1]  # size of augmented data\n",
    "\n",
    "batch_size = tf1.placeholder(tf.int32, ())\n",
    "\n",
    "N_net = 10 # number of NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_nn = rl.nn.MLP(\n",
    "    n_input=latent_dim + target_dim, n_output=joint_dim, n_hidden=[200, 200],\n",
    "    act_fct=tf.nn.relu, batch_size_svi=N_net\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the input to generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise to feed generator\n",
    "eps = tf.random.normal([tf.cast(batch_size/N_net, tf.int32), latent_dim], \n",
    "    dtype=tf.float32, mean=0., stddev=1.0, name='epsilon')\n",
    "\n",
    "# target parameters to feed generator\n",
    "batch_target = tf1.placeholder(tf.float32, (None, target_dim))\n",
    "\n",
    "# to feed main generator noise + parameters samples\n",
    "eps_conc = tf.concat([eps, batch_target], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main generator\n",
    "samples_q = tf.reshape(gen_nn.pred(eps_conc) + tf.constant(chain.mean_pose)[None], (-1,joint_dim))\n",
    "# compute links of main generator\n",
    "samples_x = chain.xs(samples_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input with varying noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second generator for using the model later with given targets and changing noise\n",
    "std_guided = tf1.placeholder(tf.float32, ())\n",
    "eps_guided = tf.random.normal([batch_size, latent_dim], \n",
    "    dtype=tf.float32, mean=0., stddev=std_guided, name='epsilon')\n",
    "\n",
    "guided_samples_q = tf.reshape(gen_nn.pred(\n",
    "    tf.concat([eps_guided, batch_target], axis=-1)) + tf.constant(chain.mean_pose)[None], (-1,joint_dim))\n",
    "guided_samples_x = chain.xs(guided_samples_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discr_nn = rl.nn.MLP(\n",
    "    n_input=augmented_dim, n_output=1, n_hidden=[20, 20],\n",
    "    act_fct=tf.nn.relu\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data in\n",
    "batch_x = tf1.placeholder(tf.float32, (None, augmented_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability to belong to data for data and samples\n",
    "d_fake = discr_nn.pred(q_augmented(samples_q))[:, 0]\n",
    "d_true = discr_nn.pred(batch_x)[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main loss function (discriminator + generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_d = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(d_true), logits=d_true) + \\\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(d_fake), logits=d_fake)\n",
    "loss_d = tf.reduce_sum(loss_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generative loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=tf.ones_like(d_fake), logits=d_fake)\n",
    "loss = tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_target_std = tf1.placeholder(tf.float32, ()) \n",
    "\n",
    "p_target = ds.MultivariateNormalFullCovariance(\n",
    "    tf.reshape(tf.ones((N_net, 1,1))*batch_target[None],(-1, target_dim)), p_target_std**2 * tf.eye(3))\n",
    "    \n",
    "samples_target_proj = fs[1](samples_q)\n",
    "\n",
    "loss_target = tf.reduce_sum(-p_target.log_prob(samples_target_proj))\n",
    "lmbda_target = tf1.placeholder(tf.float32, ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constraints Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbda_constraints = tf1.placeholder(tf.float32, ())\n",
    "joint_limits = tf.constant(chain.joint_limits, dtype=tf.float32)\n",
    "joint_limits_std = 0.05\n",
    "joint_limits_temp = 1.\n",
    "\n",
    "joint_limits_exp = ds.SoftUniformNormalCdf(\n",
    "    low=joint_limits[:, 0],\n",
    "    high=joint_limits[:, 1],\n",
    "    std=joint_limits_std,\n",
    "    temp=joint_limits_temp,\n",
    "    reduce_axis=-1\n",
    ")\n",
    "joint_limit_constraints = tf.reduce_mean(-joint_limits_exp.log_prob(samples_q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_gen =  lmbda_target * loss_target + lmbda_constraints*joint_limit_constraints + loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = tf1.placeholder(tf.float32, ())\n",
    "opt = tf1.train.AdamOptimizer\n",
    "\n",
    "optimizer = opt(learning_rate=rate)\n",
    "optimizer_d = opt(learning_rate=rate)\n",
    "\n",
    "gen_var = gen_nn.vec_weights\n",
    "train = optimizer.minimize(loss_gen, var_list=gen_var)\n",
    "train_d = optimizer_d.minimize(loss_d, var_list=discr_nn.vec_weights)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf1.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_batch_size = 100\n",
    "alpha = 0.5\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(10000):\n",
    "    try:\n",
    "        for j in range(5):\n",
    "            # train discriminative_network\n",
    "            _x = get_batch(_batch_size=_batch_size)\n",
    "#             _x =np.copy(_x) + np.random.normal(0., 0.05, (_batch_size, augmented_dim))\n",
    "            feed_dict = {\n",
    "                lmbda_target: .1,\n",
    "                lmbda_constraints: 1.,\n",
    "                p_target_std: 0.05,\n",
    "                batch_x: _x,\n",
    "                batch_size: _batch_size,\n",
    "                rate : 0.002 * alpha\n",
    "            }\n",
    "            feed_dict[batch_target] = get_target_batch(_batch_size=int(_batch_size/N_net))\n",
    "\n",
    "            _ = sess.run([train_d], feed_dict=feed_dict)\n",
    "        \n",
    "        feed_dict[rate] = 0.001 * alpha\n",
    "        # train generative_network\n",
    "        _, _loss, _loss_target, _loss_d = sess.run(\n",
    "            [train, loss, loss_target, loss_d], feed_dict=feed_dict)\n",
    "        \n",
    "        if not i % 10:\n",
    "            display.clear_output(wait=True)\n",
    "            print('Step %i\\t, Loss gen: %f\\t, loss discr %f, loss target %f' % (i, _loss, _loss_d, _loss_target))\n",
    "    except KeyboardInterrupt:\n",
    "        toc = time.time()\n",
    "        print(toc-tic)\n",
    "        break\n",
    "        \n",
    "toc = time.time()\n",
    "print(toc-tic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To save model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "saver = tf1.train.Saver()\n",
    "# save_path = saver.save(sess, \"data/panda_gan_target_constrained3_without_data.ckpt\")\n",
    "save_path = saver.save(sess, \"data/panda_gan_target_constrained_fewdata.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### To load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf1.train.Saver()\n",
    "saver.restore(sess, \"data/panda_gan_target_constrained.ckpt\")\n",
    "#saver.restore(sess, \"data/panda_gan_target_constrained_fewdata.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "# _targets = np.random.multivariate_normal(\n",
    "                        # [0.4, 0., 0.2], 0.3 ** 2 * np.eye(3), (n,))\n",
    "_targets = get_target_batch(_batch_size=int(n/N_net))\n",
    "\n",
    "_samples_q,_samples_xs = sess.run([\n",
    "        samples_q, samples_x], {batch_size: n, batch_target: _targets})\n",
    "    \n",
    "_data_q = get_batch(n, augmented=False) \n",
    "_data_q += np.random.normal(0., 0.05, (n, 7))\n",
    "_data_xs =  sess.run(x_eval, feed_dict={q_eval: _data_q})\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(10, 5))\n",
    "\n",
    "dims = [0, 2]\n",
    "ax[0].set_title('data')\n",
    "chain.plot(_data_xs, dim=dims, alpha=0.2, color='k', ax=ax[0])\n",
    "ax[1].set_title('samples')\n",
    "chain.plot(_samples_xs, dim=dims, alpha=0.2, color='k', ax=ax[1])\n",
    "\n",
    "ax[1].plot(_targets[:, dims[0]], _targets[:, dims[1]], 'rx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot target distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 1\n",
    "\n",
    "_target = get_target_batch(cut=5000, _batch_size=1)[0]\n",
    "# _target = np.random.multivariate_normal([0.4, 0., 0.2], 0.2 ** 2 * np.eye(3))\n",
    "_targets = _target[None] * np.ones((n, 1)) \n",
    "\n",
    "_samples_x = guided_samples_x.eval({batch_size: n, std_guided: .1, batch_target: _targets})\n",
    "\n",
    "_data_q = get_batch(n, augmented=False)\n",
    "_data_xs =  sess.run(x_eval, feed_dict={q_eval: _data_q})\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(10, 3))\n",
    "\n",
    "for i in range(2):\n",
    "    dims = [i, 2]\n",
    "    chain.plot(_samples_x, dim=dims, alpha=0.2, color='k', ax=ax[i])\n",
    "    ax[i].plot(_targets[0][dims[0]], _targets[0][dims[1]], 'rs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save figure"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(ncols=1, sharex=True, sharey=True, figsize=(6,8))\n",
    "\n",
    "dims = [1, 2]\n",
    "chain.plot(_samples_x, dim=dims, alpha=0.2, color='k', ax=ax)\n",
    "ax.plot(_targets[0][dims[0]], _targets[0][dims[1]], 'ro', markersize=20)\n",
    "plt.axis('off')\n",
    "plt.savefig('data/panda_config12.png', bbox_inches='tight', pad_inches=0)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_targets = np.array([[-0.5, 0, 0.6]])\n",
    "\n",
    "_samples_q, _samples_x = sess.run([guided_samples_q, guided_samples_x], {batch_size: len(_targets), std_guided: .1, batch_target: _targets})\n",
    "\n",
    "print(_samples_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Use it for Projection, IK and Inverse Kinematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Pybullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_client_id = p.connect(p.DIRECT)\n",
    "p.setPhysicsEngineParameter(enableFileCaching=0)\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "p.configureDebugVisualizer(p.COV_ENABLE_GUI,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.resetSimulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Robot & environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_urdf = DATA_PATH + '/urdf/panda_arm.urdf'\n",
    "robot_id = p.loadURDF(fileName=robot_urdf)\n",
    "dof = p.getNumJoints(robot_id)\n",
    "pb_joint_indices = np.arange(7)\n",
    "joint_limits = get_joint_limits(robot_id,pb_joint_indices)\n",
    "mean_pose = 0.5*(joint_limits[0]+joint_limits[1])\n",
    "\n",
    "plane_id = p.loadURDF('plane.urdf')\n",
    "p.resetBasePositionAndOrientation(plane_id, (0,0,-.5), (0,0,0,1))\n",
    "\n",
    "table_square_id = p.loadURDF('table_square/table_square.urdf')\n",
    "p.resetBasePositionAndOrientation(table_square_id, (0.,0,-0.64), (0, 0, 0.7071068, 0.7071068))\n",
    "\n",
    "table_id = p.loadURDF('table/table.urdf')\n",
    "p.resetBasePositionAndOrientation(table_id, (.7,0,-0.5), (0, 0, 0.7071068, 0.7071068))\n",
    "\n",
    "shelf_urdf = DATA_PATH + '/urdf/bookshelf_simple_collision.urdf'\n",
    "shelf_id = p.loadURDF(fileName=shelf_urdf)\n",
    "p.resetBasePositionAndOrientation(shelf_id, (-0.6,0.6,-0.5), (0, 0, 0, 1.))\n",
    "\n",
    "#for visualizing the desired target\n",
    "_,_,ball_id = create_primitives(radius=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model in pinocchio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_urdf = DATA_PATH + '/urdf/panda_arm.urdf';\n",
    "rmodel = pin.buildModelFromUrdf(robot_urdf)\n",
    "rdata = rmodel.createData()\n",
    "\n",
    "pin_frame_names = [f.name for f in rmodel.frames]\n",
    "ee_frame_id = rmodel.getFrameId('panda_hand2')\n",
    "ee_frame_id2 = rmodel.getFrameId('panda_hand2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define standard functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeJacobian_std = partial(computeJacobian, rmodel, rdata, ee_frame_id)\n",
    "computePose_std = partial(computePose, rmodel, rdata, ee_frame_id)\n",
    "set_q_std = partial(set_q,robot_id, pb_joint_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Cost Model for projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_target = []\n",
    "for i in range(data.shape[0]):\n",
    "    pos, ori = computePose_std(data[i])\n",
    "    data_target += [pos]\n",
    "data_target = np.array(data_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_ik = np.eye(3)\n",
    "rotation_ik_shelf = w2mat((0,0,np.pi/2))\n",
    "rotation_ik_table = w2mat((0,0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_cost_new = CostFrameRotationSE3FloatingBaseNew(rmodel, rdata, rotation_ik, ee_frame_id, weight=np.array([1.,1.,0.]))\n",
    "ori_cost_new_shelf = CostFrameRotationSE3FloatingBaseNew(rmodel, rdata, rotation_ik_shelf, ee_frame_id, weight=np.array([1.,1.,1.]))\n",
    "ori_cost_new_table = CostFrameRotationSE3FloatingBaseNew(rmodel, rdata, rotation_ik_table, ee_frame_id, weight=np.array([1.,1.,1.]))\n",
    "pose_cost_new = CostFrameTranslationFloatingBaseNew(rmodel, rdata, np.zeros(3), ee_frame_id, weight=np.array([1.,1.,1.]))\n",
    "bound_cost_new = CostBoundNew(joint_limits, 1e-4)\n",
    "posture_cost_new = CostPostureNew(rmodel, rdata, mean_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for projection\n",
    "cost_sum_new = CostSumNew(rmodel, rdata)\n",
    "cost_sum_new.addCost(ori_cost_new, 20., 'ori_cost', 1e-4)\n",
    "cost_sum_new.addCost(bound_cost_new, 20., 'joint_limit', 1e-3)\n",
    "\n",
    "#for IK with horizontal gripper constraint\n",
    "cost_sum_new_ik = CostSumNew(rmodel, rdata)\n",
    "cost_sum_new_ik.addCost(pose_cost_new,50., 'pose_cost', 1e-3)\n",
    "cost_sum_new_ik.addCost(ori_cost_new, 30., 'ori_cost', 1e-3)\n",
    "cost_sum_new_ik.addCost(bound_cost_new, 30., 'joint_limit', 1e-3)\n",
    "\n",
    "#for IK with gripper facing the shelf\n",
    "cost_sum_new_ik_shelf = CostSumNew(rmodel, rdata)\n",
    "cost_sum_new_ik_shelf.addCost(pose_cost_new,50., 'pose_cost', 1e-3)\n",
    "cost_sum_new_ik_shelf.addCost(ori_cost_new_shelf, 30., 'ori_cost', 1e-3)\n",
    "cost_sum_new_ik_shelf.addCost(bound_cost_new, 30., 'joint_limit', 1e-3)\n",
    "\n",
    "#for IK with gripper facing the table\n",
    "cost_sum_new_ik_table = CostSumNew(rmodel, rdata)\n",
    "cost_sum_new_ik_table.addCost(pose_cost_new,50., 'pose_cost', 1e-3)\n",
    "cost_sum_new_ik_table.addCost(ori_cost_new_table, 30., 'ori_cost', 1e-3)\n",
    "cost_sum_new_ik_table.addCost(bound_cost_new, 30., 'joint_limit', 1e-3)\n",
    "\n",
    "#secondary task: posture regularization\n",
    "cost_sum_new2 = CostSumNew(rmodel, rdata)\n",
    "cost_sum_new2.addCost(posture_cost_new, 1. , 'posture', 1e3)\n",
    "\n",
    "robot_projector = TalosCostProjectorNew(cost_sum_new, rmodel, rdata, cost2 = cost_sum_new2, bounds = joint_limits)\n",
    "robot_ik_solver = TalosCostProjectorNew(cost_sum_new_ik, rmodel, rdata, cost2 = cost_sum_new2, bounds = joint_limits)\n",
    "robot_ik_solver_shelf = TalosCostProjectorNew(cost_sum_new_ik_shelf, rmodel, rdata, cost2 = None, bounds = joint_limits)\n",
    "robot_ik_solver_table = TalosCostProjectorNew(cost_sum_new_ik_table, rmodel, rdata, cost2 = None, bounds = joint_limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gan_sampler():\n",
    "    def __init__(self, target_sampler):\n",
    "        self.target_sampler = target_sampler\n",
    "       \n",
    "    def sample(self, N=1, _targets = None, var = 1. ):\n",
    "        if _targets is None:\n",
    "            _targets = self.target_sampler.sample(N)\n",
    "            \n",
    "        _samples_q = sess.run([guided_samples_q], {batch_size: len(_targets), std_guided: var, batch_target: _targets})\n",
    "        qnew = []\n",
    "        for i in range(N):\n",
    "            idx = np.random.randint(N_net)\n",
    "            q = _samples_q[0][idx*N+i]\n",
    "            qnew += [q]\n",
    "        \n",
    "        return np.array(qnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_area = np.array([[-0.6, -0.6 , -0.1], [0.6, 0.6, 1.0]])\n",
    "#workspace_area = np.array([[-0.8, -0.8 , 0.1], [0.8, 0.8, 1.0]])\n",
    "target_sampler = sampler(workspace_area)\n",
    "rob_gan_sampler = gan_sampler(target_sampler)\n",
    "rob_simple_sampler = sampler(joint_limits)\n",
    "rob_col_checker = col_checker(robot_id, pb_joint_indices, [ plane_id, shelf_id, table_id, table_square_id])\n",
    "rob_interpolator = interpolator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try the projection operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = rob_simple_sampler.sample().flatten()\n",
    "res = robot_projector.project(q)\n",
    "q, success, func_calls = res['q'], res['stat'], res['nfev']\n",
    "set_q_std(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_collide = True\n",
    "while is_collide is True:\n",
    "    q = rob_simple_sampler.sample().flatten()\n",
    "    res = robot_projector.project(q)\n",
    "    q, success, func_calls = res['q'], res['stat'], res['nfev']\n",
    "    #print(projector_constraint.project(q))\n",
    "    is_collide = rob_col_checker.check_collision(q)\n",
    "set_q_std(q.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [rob_simple_sampler, rob_gan_sampler]\n",
    "method_names = ['Random', 'GAN']\n",
    "\n",
    "samples = rob_gan_sampler.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "data = dict()\n",
    "for m in range(len(methods)):\n",
    "    comp_times = []\n",
    "    success = []\n",
    "    fevals = []\n",
    "    for i in range(N):\n",
    "        idx = np.random.randint(500)\n",
    "        if method_names[m] == 'GAN':\n",
    "            q = samples[idx]\n",
    "        else:\n",
    "            q = rob_simple_sampler.sample().flatten()\n",
    "        tic = time.time()\n",
    "        res = robot_projector.project(q)\n",
    "        toc = time.time()\n",
    "        comp_times += [toc-tic]\n",
    "        success += [res['stat']]\n",
    "        fevals += [res['nfev']]\n",
    "    data[method_names[m]] = [comp_times, success, fevals]\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in range(len(methods)):\n",
    "    print('& ' + method_names[m], end=' ')\n",
    "    comp_times, success, fevals = data[method_names[m]]\n",
    "    comp_times = np.array(comp_times)\n",
    "    fevals = np.array(fevals)\n",
    "\n",
    "    print('& {0:.1f} &  {1:.1f} $\\pm$ {2:.1f} &  {3:.1f} $\\pm$ {4:.1f} &  {5:.1f} $\\pm$ {6:.1f}'.format(np.sum(success)*100./N, np.mean(comp_times)*1000, np.std(comp_times)*1000, np.mean(comp_times[success])*1000, np.std(comp_times[success])*1000,np.mean(fevals[success]), np.std(fevals[success])), end = ''),\n",
    "    print('\\\\\\\\ ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare IK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ik_workspace_area = np.array([[0.4, -0.5 , 0.5], [0.7, 0.5, .8]])\n",
    "ik_target_sampler = sampler(ik_workspace_area)\n",
    "#goal_poses = data_target[:1000]\n",
    "goal_poses = ik_target_sampler.sample(1000)\n",
    "for i in range(len(goal_poses)):\n",
    "    goal_poses[i] = clip_bounds(goal_poses[i], ik_workspace_area)\n",
    "qs = rob_gan_sampler.sample(N=1000, _targets=goal_poses, var = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try IK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(1000)\n",
    "goal_pos = goal_poses[idx]\n",
    "if np.linalg.norm(goal_pos[:2]) < 0.3:\n",
    "    print('Likely to fail')\n",
    "\n",
    "if np.linalg.norm(goal_pos) > 0.8:\n",
    "    print('Likely to fail')\n",
    "    \n",
    "p.resetBasePositionAndOrientation(ball_id, goal_pos, np.array([0,0,0,1]))\n",
    "\n",
    "robot_ik_solver.cost.costs['pose_cost'].cost.desired_pose = goal_pos\n",
    "q = qs[idx]\n",
    "#q = rob_simple_sampler.sample().flatten()\n",
    "set_q_std(q)\n",
    "\n",
    "# res = robot_ik_solver.project(q, maxiter = 100)\n",
    "# print(res['stat'])\n",
    "# set_q_std(res['q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "data = dict()\n",
    "n_retry = 2\n",
    "for m in range(len(methods)):\n",
    "    comp_times = []\n",
    "    success = []\n",
    "    fevals = []\n",
    "    for i in range(N):\n",
    "        stat = False\n",
    "        idx = np.random.randint(1000)\n",
    "        goal_pos = goal_poses[idx]\n",
    "\n",
    "        stat = True\n",
    "        robot_ik_solver.cost.costs['pose_cost'].cost.desired_pose = goal_pos\n",
    "        for i in range(n_retry):\n",
    "            if method_names[m] == 'GAN':\n",
    "                q = qs[idx]\n",
    "            else:\n",
    "                q = rob_simple_sampler.sample().flatten()\n",
    "\n",
    "            tic = time.time()\n",
    "            res = robot_ik_solver.project(q)\n",
    "            toc = time.time()\n",
    "            if res['stat']: break\n",
    "        comp_times += [toc-tic]\n",
    "        success += [res['stat']]\n",
    "        fevals += [res['nfev']]\n",
    "    data[method_names[m]] = [comp_times, success, fevals]\n",
    "clear_output()\n",
    "\n",
    "for m in range(len(methods)):\n",
    "    print('& ' + method_names[m], end=' ')\n",
    "    comp_times, success, fevals = data[method_names[m]]\n",
    "    comp_times = np.array(comp_times)\n",
    "    fevals = np.array(fevals)\n",
    "\n",
    "    print('& {0:.1f} &  {1:.1f} $\\pm$ {2:.1f} &  {3:.1f} $\\pm$ {4:.1f} &  {5:.1f} $\\pm$ {6:.1f}'.format(np.sum(success)*100./N, np.mean(comp_times)*1000, np.std(comp_times)*1000, np.mean(comp_times[success])*1000, np.std(comp_times[success])*1000,np.mean(fevals[success]), np.std(fevals[success])), end = ''),\n",
    "    print('\\\\\\\\ ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup different RRTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_goal():\n",
    "    def __init__(self, sampler, projector_pose, x_goal):\n",
    "        self.projector_pose = projector_pose\n",
    "        self.sampler = sampler\n",
    "        self.x_goal = x_goal\n",
    "        \n",
    "    def check(self, q):\n",
    "        pos, ori = computePose_std(q)\n",
    "        if np.linalg.norm(pos - x_goal) < self.g_tol:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def sample(self):\n",
    "        sample = self.sampler.sample()\n",
    "        self.projector_pose.cost.costs['pose_cost'].cost.desired_pose[:3] = self.x_goal\n",
    "        res = self.projector_pose.project(sample.flatten())\n",
    "        proj_sample, cost, success = res['q'], res['feval'], res['stat']\n",
    "        return proj_sample, success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_valid_q():\n",
    "    is_collide = True\n",
    "    status = False\n",
    "    while is_collide is True or status is False:\n",
    "        q = rob_simple_sampler.sample().flatten()\n",
    "        res = robot_projector.project(q)\n",
    "        q, status, func_calls_  = res['q'], res['stat'], res['nfev']\n",
    "        is_collide = rob_col_checker.check_collision(q.flatten())\n",
    "        \n",
    "    return q.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_goal_states(N, goal):\n",
    "    q_goals = []\n",
    "\n",
    "    for i in range(N):\n",
    "        while(1):\n",
    "            q_goal, success = goal.sample()\n",
    "            if success and rob_col_checker.check_collision(q_goal) is False:\n",
    "                q_goals += [q_goal]\n",
    "                break\n",
    "                \n",
    "    return q_goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_rrt =cRRT(7, rob_simple_sampler,  rob_col_checker, rob_interpolator, robot_projector)\n",
    "gan_rrt =cRRT(7, rob_gan_sampler,  rob_col_checker, rob_interpolator, robot_projector)\n",
    "hybrid_sampler = HybridSampler(rob_simple_sampler, rob_gan_sampler, p_random=0.5)\n",
    "hybrid_rrt =cRRT(7, hybrid_sampler,  rob_col_checker, rob_interpolator, robot_projector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform N Tasks to compare the runtime of RRTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_goal_bottom_left = np.array([-0.4, 0.65, 0.2])\n",
    "x_goal_top_left = np.array([-0.4, 0.65, 0.54])\n",
    "x_goal_bottom_right = np.array([-0.02, 0.65, 0.2])\n",
    "x_goal_top_right = np.array([-0.02, 0.65, 0.54])\n",
    "\n",
    "x_goals = []\n",
    "x_goals += [x_goal_bottom_left]\n",
    "x_goals += [x_goal_top_left]\n",
    "x_goals += [x_goal_bottom_right]\n",
    "x_goals += [x_goal_top_right]\n",
    "\n",
    "x_inits = np.array([[0.5, -0.25, 0.2],\n",
    "                    [0.5, 0.25, 0.2],\n",
    "                    [0.7, -0.25, 0.2],\n",
    "                    [0.7, 0.25, 0.2]])\n",
    "\n",
    "N = 10\n",
    "p_goal = 0.5\n",
    "p_random_hybrid = 0.2\n",
    "p_random_gan = 0.0\n",
    "p_random_uniform = 1.0\n",
    "max_extension_steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# recorded runtime\n",
    "T1 = []\n",
    "T2 = []\n",
    "T3 = []\n",
    "# projection iterations\n",
    "P1 = []\n",
    "P2 = []\n",
    "P3 = []\n",
    "# extension steps\n",
    "E1 = []\n",
    "E2 = []\n",
    "E3 = []\n",
    "# failure count\n",
    "S1 = 0\n",
    "S2 = 0\n",
    "S3 = 0\n",
    "\n",
    "# retry count\n",
    "R1 = []\n",
    "R2 = []\n",
    "R3 = []\n",
    "\n",
    "E_F1 = []\n",
    "E_F2 = []\n",
    "E_F3 = []\n",
    "\n",
    "q_init_ = []\n",
    "\n",
    "N = 10\n",
    "for k in range(N):\n",
    "    print('Iteration number' + str(k))\n",
    "    print(np.mean(T1),np.mean(T2),np.mean(T3))\n",
    "    print(np.mean(P1),np.mean(P2),np.mean(P3))\n",
    "    print(np.mean(E1),np.mean(E2),np.mean(E3))\n",
    "    print(np.mean(R1),np.mean(R2),np.mean(R3))\n",
    "    print(np.mean(S1),np.mean(S2),np.mean(S3))\n",
    "\n",
    "    #q_init = sample_valid_q()\n",
    "    #q_init_ += [q_init]\n",
    "    \n",
    "    rob_simple_init = simple_goal(rob_gan_sampler, robot_ik_solver_table, x_inits[np.random.randint(4)])\n",
    "    q_init = sample_goal_states(1, rob_simple_init)[0]\n",
    "\n",
    "    rand_idx = np.random.randint(4)\n",
    "    rob_simple_goal = simple_goal(rob_gan_sampler, robot_ik_solver_shelf, x_goals[rand_idx])\n",
    "    q_goals = sample_goal_states(10, rob_simple_goal)\n",
    "    print('Uniform...')    \n",
    "    \n",
    "    #t, proj_iter, ext_steps, success, n_retry = evaluate_cRRT(q_init, q_goals, p_goal, p_random_hybrid, max_extension_steps)\n",
    "    traj, proj_iter, ext_steps, success, n_retry, t , path = standard_rrt.plan(q_init, q_goals,max_extension_steps=500) \n",
    "    if success:\n",
    "        T1 += [t]\n",
    "        P1 += [proj_iter]\n",
    "        E1 += [ext_steps]\n",
    "        S1 += 0\n",
    "        R1 += [n_retry]\n",
    "        E_F1 += [standard_rrt.extend_nfevs]\n",
    "    else:\n",
    "        T1 += [0]\n",
    "        P1 += [0]\n",
    "        E1 += [0]\n",
    "        S1 += 1\n",
    "        R1 += [0]\n",
    "    \n",
    "    print('GAN...')\n",
    "    \n",
    "    traj, proj_iter, ext_steps, success, n_retry, t, path  = gan_rrt.plan(q_init, q_goals,max_extension_steps=500) \n",
    "    if success:\n",
    "        T2 += [t]\n",
    "        P2 += [proj_iter]\n",
    "        E2 += [ext_steps]\n",
    "        S2 += 0\n",
    "        R2 += [n_retry]\n",
    "        E_F2 += [gan_rrt.extend_nfevs]\n",
    "        \n",
    "    else:\n",
    "        T2 += [0]\n",
    "        P2 += [0]\n",
    "        E2 += [0]\n",
    "        S2 += 1\n",
    "        R2 += [0]\n",
    "        \n",
    "    print('Hybrid...')\n",
    "    traj, proj_iter, ext_steps, success, n_retry, t, path  = hybrid_rrt.plan(q_init, q_goals,max_extension_steps=500) \n",
    "    if success:\n",
    "        T3 += [t]\n",
    "        P3 += [proj_iter]\n",
    "        E3 += [ext_steps]\n",
    "        S3 += 0\n",
    "        R3 += [n_retry]\n",
    "        E_F3 += [hybrid_rrt.extend_nfevs]\n",
    "        \n",
    "    else:\n",
    "        T3 += [0]\n",
    "        P3 += [0]\n",
    "        E3 += [0]\n",
    "        S3 += 1\n",
    "        R3 += [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i += 1\n",
    "plt.hist(E_F1[i], color='r', alpha=0.7)\n",
    "plt.figure()\n",
    "plt.hist(E_F2[i], color='g', alpha=0.7)\n",
    "plt.figure()\n",
    "plt.hist(E_F3[i], color='b', alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.concatenate(E_F1)\n",
    "a2 = np.concatenate(E_F2)\n",
    "a3 = np.concatenate(E_F3)\n",
    "print(np.mean(a1), np.std(a1))\n",
    "print(np.mean(a2), np.std(a2))\n",
    "print(np.mean(a3), np.std(a3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.concatenate(E_F1), color='r', alpha=0.7)\n",
    "plt.figure()\n",
    "plt.hist(np.concatenate(E_F2), color='g', alpha=0.7)\n",
    "plt.figure()\n",
    "plt.hist(np.concatenate(E_F3), color='b', alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_failure(T):\n",
    "    Tn = list(T)\n",
    "    i = 0\n",
    "    count_del = 0\n",
    "    while i < len(Tn):\n",
    "        if Tn[i] == 0:\n",
    "            del Tn[i]\n",
    "            count_del += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return Tn, count_del\n",
    "\n",
    "T1n,d1 = remove_failure(T1)\n",
    "T2n,d2 = remove_failure(T2)\n",
    "T3n,d3 = remove_failure(T3)\n",
    "print(d1,d2,d3)\n",
    "\n",
    "P1n,_ = remove_failure(P1)\n",
    "P2n,_ = remove_failure(P2)\n",
    "P3n,_ = remove_failure(P3)\n",
    "\n",
    "E1n,_ = remove_failure(E1)\n",
    "E2n,_ = remove_failure(E2)\n",
    "E3n,_ = remove_failure(E3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('& Random & {0:.1f} & {1:.2f} $\\pm$ {2:.2f} & {3:.1f} $\\pm$ {4:.1f} & {5:.1f} $\\pm$ {6:.1f} \\\\\\\\'.format((len(T3)-d1)*100./len(T3), np.mean(T1n), np.std(T1n), np.mean(P1n), np.std(P1n), np.mean(E1n), np.std(E1n)))\n",
    "print('& GAN & {0:.1f} & {1:.2f} $\\pm$ {2:.2f} & {3:.1f} $\\pm$ {4:.1f} & {5:.1f} $\\pm$ {6:.1f}  \\\\\\\\'.format((len(T3)-d2)*100./len(T3),    np.mean(T2n), np.std(T2n), np.mean(P2n), np.std(P2n), np.mean(E2n), np.std(E2n)))\n",
    "print('& Hybrid & {0:.1f} & {1:.2f} $\\pm$ {2:.2f} & {3:.1f} $\\pm$ {4:.1f} & {5:.1f} $\\pm$ {6:.1f}  \\\\\\\\'.format((len(T3)-d3)*100./len(T3), np.mean(T3n), np.std(T3n), np.mean(P3n), np.std(P3n), np.mean(E3n), np.std(E3n)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "data['T'] = [T1,T2,T3]\n",
    "data['P'] = [P1,P2,P3]\n",
    "data['E'] = [E1,E2,E3]\n",
    "data['d'] = [d1,d2,d3]\n",
    "np.save('data/panda_res.npy',data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
